{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaksimReznicenko/Maksim_Reznichenko/blob/main/%D0%A0%D0%956_2_%22LangChain__LangModel%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangChain**\n"
      ],
      "metadata": {
        "id": "hWTEQor1zbaF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMBEVFbey8qi"
      },
      "outputs": [],
      "source": [
        "#Установим необходимые библиотеки:\n",
        "!pip install -qU langchain\n",
        "!pip install -qU langchain-openai\n",
        "!pip install -qU langchain-anthropic\n",
        "!pip install -qU langchain-community\n",
        "!pip install -qU wikipedia\n",
        "\n",
        "# или !pip install -qU langchain langchain-openai wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://platform.openai.com/account/api-keys\n",
        "\n",
        "https://smith.langchain.com/\n",
        "\n",
        "https://www.youtube.com/watch?v=VOwI5RsPyzc - Как использовать прокси в браузере"
      ],
      "metadata": {
        "id": "UzFkPBht1N9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=\"добавляем свой ключ\"\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"добавляем свой ключ\""
      ],
      "metadata": {
        "id": "yOOxhmlg076O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(\n",
        "\tmodel=\"gpt-4o-mini\",\n",
        "\ttemperature=0.7,\n",
        "\tmax_tokens=1000\n",
        "\t)"
      ],
      "metadata": {
        "id": "bupVKdV5Fa5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"ANTHROPIC_API_KEY\"] =\"123\"\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "model_claude = ChatAnthropic(model=\"claude-3-sonnet-20240229\")"
      ],
      "metadata": {
        "id": "0noZbeBPMs8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "SystemMessage(content=\"Translate the following from English into Italian\"),\n",
        "HumanMessage(content=\"hi!\"),\n",
        "\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "id": "_WhFAdB2HKfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "result = model.invoke(messages)\n",
        "parser.invoke(result)"
      ],
      "metadata": {
        "id": "V75JKhn8IPIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = model | parser\n",
        "chain.invoke(messages)"
      ],
      "metadata": {
        "id": "Jxzz5hWEIklK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим LangSmith!"
      ],
      "metadata": {
        "id": "lFLR_cV3KCps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#получим стоимость запроса\n",
        "from langchain_community.callbacks.manager import get_openai_callback\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    result = chain.invoke(messages)\n",
        "    print(cb)"
      ],
      "metadata": {
        "id": "F80r1WdOIkW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#проверим другую модель\n",
        "model4 = ChatOpenAI(model=\"gpt-4o\")\n",
        "chain4 = model4 | parser\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    result = chain4.invoke(messages)\n",
        "    print(cb)"
      ],
      "metadata": {
        "id": "3EInDdyVMPoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total Cost (USD): {cb.total_cost}$\")"
      ],
      "metadata": {
        "id": "Cy_O5MNJO_K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Используем шаблон промпта\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following into {language}:\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ],
      "metadata": {
        "id": "CYmyhaUXNF3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "DwkMxAq3J3VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#доступ к сообщениям\n",
        "result.to_messages()#[0]"
      ],
      "metadata": {
        "id": "vlxqotaojHWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_messages()[0]"
      ],
      "metadata": {
        "id": "XhFe2oCANF0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#добавляем шаблон\n",
        "chain = prompt_template | model | parser"
      ],
      "metadata": {
        "id": "5aieAglgNFyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})"
      ],
      "metadata": {
        "id": "DMkl79u2NFvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Пример использования\n",
        "system_template = \"Ты - полезный бот массовик затейник. Ты знаешь много способов провести интересно время. Тебя зовут {name}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template),\n",
        "    (\"user\", \"Меня зовут Михаил. Я люблю карточные игры, особенно покер. Хочу научиться играть в преферанс. \\\n",
        "        Не люблю гулять на свежем воздухе\"),\n",
        "\n",
        "        (\"user\", \"{user_input}\")]\n",
        "\n",
        ")\n",
        "\n",
        "result = prompt_template.invoke({\"name\": \"Бодрый друг\", \"user_input\": \"На улице солнечно. Чем посоветуешь заняться?\"})\n",
        "\n",
        "chain = prompt_template | model | parser\n",
        "data = {\"name\": \"Бодрый друг\", \"user_input\": \"На улице солнечно. Чем посоветуешь заняться?\"}\n",
        "chain.invoke(data)"
      ],
      "metadata": {
        "id": "gftkPdKHNFp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ответ в режиме стрима\n",
        "chat = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7,\n",
        "    )\n",
        "for chunk in chat.stream(\"Напиши тезисы про важность промпт-инжиниринга\"):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "BjIgaqKMD0It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " LangChain. Взаимодействие с языковыми моделями: Часть 2"
      ],
      "metadata": {
        "id": "hbjSbI8jjXFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "# Определение Pydantic модели для структурированного вывода\n",
        "class Person(BaseModel):\n",
        "    name: str = Field(description=\"Полное имя человека\")\n",
        "    age: int = Field(description=\"Возраст человека\")\n",
        "    hobbies: List[str] = Field(description=\"Список хобби\")\n",
        "\n",
        "# Создание парсера\n",
        "parser = PydanticOutputParser(pydantic_object=Person)\n",
        "parser.get_format_instructions().encode('utf-8').decode('unicode-escape')"
      ],
      "metadata": {
        "id": "eiFeVotRVAzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание шаблона промпта\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Extract structured information from the following text:\\n{text}\\n{format_instructions}\",\n",
        "    input_variables=[\"text\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "# Создание цепочки\n",
        "chain = prompt | model | parser\n",
        "\n",
        "text = \"\"\"\n",
        "Иван Иванов - 32-летний инженер-программист. Он любит играть на гитаре,\n",
        "ходить в походы по выходным и экспериментировать с новыми кулинарными рецептами.\n",
        "\"\"\"\n",
        "\n",
        "result = chain.invoke({\"text\": text})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "HnGeVKBGDzqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Name: {result.name}\")\n",
        "print(f\"Age: {result.age}\")\n",
        "print(f\"Hobbies: {', '.join(result.hobbies)}\")"
      ],
      "metadata": {
        "id": "ZPtYSPqBUfS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_debug\n",
        "set_debug(True)\n",
        "result = chain.invoke({\"text\": text})\n",
        "set_debug(True)"
      ],
      "metadata": {
        "id": "doNN_3W8T3R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# Определите вашу модель\n",
        "llm = OpenAI(temperature=0, max_tokens=3000)\n",
        "\n",
        "# Создайте шаблон для суммаризации текста\n",
        "prompt_template = \"\"\"\n",
        "Пожалуйста, подведите итог следующему тексту:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "# Инициализация цепочки с использованием LLM\n",
        "prompt = PromptTemplate(input_variables=[\"text\"], template=prompt_template)\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Текст для суммаризации\n",
        "text = \"\"\"\n",
        "Команда ИИ-агентов создает гаджеты будущего без электроники!\n",
        "\n",
        "Ученые из Калифорнийского университета в Беркли разработали систему автономных ИИ-агентов,\n",
        "способную проектировать устройства на основе \"жидкостных вычислений\". Эта технология позволяет\n",
        "создавать умные гаджеты, работающие без электроники, используя лишь пневматические схемы.\n",
        "Вместо микросхем - воздушные клапаны, вместо проводов - трубки. И всё это проектирует искусственный интеллект.\n",
        "\n",
        "Система использует мультиагентный подход: один ИИ-агент генерирует идеи устройств,\n",
        "другой прорабатывает детали конструкции, третий проверяет их на реализуемость.\n",
        "В результате получаются проекты вроде йога-мата, который сам подсказывает правильные позы,\n",
        "надуваясь в нужных местах, или кресла, корректирующего вашу осанку без единого датчика.\n",
        "ИИ не просто фантазирует - он учитывает все ограничения технологии и выдает только те проекты,\n",
        "которые можно воплотить в реальность. При этом система способна за считанные минуты сгенерировать\n",
        "десятки вариантов применения технологии в различных сферах - от медицины до развлечений.\n",
        "\n",
        "Эта технология может изменить целые отрасли. Производители бытовой техники и мебели получат\n",
        "возможность создавать продукты с новым уровнем функциональности. Медицинская промышленность\n",
        "сможет разрабатывать более доступные и надёжные устройства для реабилитации. А дизайнеры получат\n",
        "мощный инструмент для воплощения самых смелых идей.\n",
        "\"\"\"\n",
        "\n",
        "# Запустите цепочку\n",
        "summary = llm_chain.run(text=text)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "xYQomPCrT3Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#используем примеры few shot\n",
        "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
        "        \"answer\": \"\"\"\n",
        "        Are follow up questions needed here: Yes.\n",
        "        Follow up: How old was Muhammad Ali when he died?\n",
        "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
        "        Follow up: How old was Alan Turing when he died?\n",
        "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
        "        So the final answer is: Muhammad Ali\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"When was the founder of craigslist born?\",\n",
        "        \"answer\": \"\"\"\n",
        "        Are follow up questions needed here: Yes.\n",
        "        Follow up: Who was the founder of craigslist?\n",
        "        Intermediate answer: Craigslist was founded by Craig Newmark.\n",
        "        Follow up: When was Craig Newmark born?\n",
        "        Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
        "        So the final answer is: December 6, 1952\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
        "        \"answer\": \"\"\"\n",
        "        Are follow up questions needed here: Yes.\n",
        "        Follow up: Who was the mother of George Washington?\n",
        "        Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
        "        Follow up: Who was the father of Mary Ball Washington?\n",
        "        Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
        "        So the final answer is: Joseph Ball\n",
        "      \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
        "        \"answer\": \"\"\"\n",
        "        Are follow up questions needed here: Yes.\n",
        "        Follow up: Who is the director of Jaws?\n",
        "        Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
        "        Follow up: Where is Steven Spielberg from?\n",
        "        Intermediate Answer: The United States.\n",
        "        Follow up: Who is the director of Casino Royale?\n",
        "        Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
        "        Follow up: Where is Martin Campbell from?\n",
        "        Intermediate Answer: New Zealand.\n",
        "        So the final answer is: No\n",
        "        \"\"\",\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "MxZseRsXNFhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
        ")\n",
        "\n",
        "print(example_prompt.format(**examples[2]))"
      ],
      "metadata": {
        "id": "Aw2ApQreaM8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {input}\",\n",
        "    input_variables=[\"input\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Ao7SC0CxaMkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model\n",
        "\n",
        "chain.invoke({\"input\": \"Как звали супруг первого президента Российской Федерации?\"})"
      ],
      "metadata": {
        "id": "8cS7y8YraMfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function calling**"
      ],
      "metadata": {
        "id": "IShWvtgEQDfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "!pip install pyowm"
      ],
      "metadata": {
        "id": "KzZnut9gjnln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки для работы с инструментами и API\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
        "from langchain.tools import Tool\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "\n",
        "# Устанавливаем ключи API для Tavily и OpenWeatherMap\n",
        "os.environ[\"TAVILY_API_KEY\"] = 'добавить API'\n",
        "os.environ[\"OPENWEATHERMAP_API_KEY\"] = 'добавить API'"
      ],
      "metadata": {
        "id": "734QhbhmaMTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Это шаблон, который будет использоваться для построения диалога между системой и пользователем\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant\"),  # Настроим систему как помощника\n",
        "        (\"placeholder\", \"{chat_history}\"),        # Место для истории чата\n",
        "        (\"human\", \"{input}\"),                      # Место для запроса от пользователя\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),     # Место для промежуточных вычислений агента\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "t0s1RQ_WaMPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаём инструмент для получения информации о погоде через OpenWeatherMap API\n",
        "weather = OpenWeatherMapAPIWrapper()\n",
        "weather_tool = Tool(\n",
        "    name=\"Weather\",\n",
        "    func=weather.run,  # Важно: эта функция будет использоваться для получения прогноза погоды\n",
        "    description=\"Useful for getting weather information\"\n",
        ")\n",
        "\n",
        "# Создаём инструмент для поиска через Tavily API (поиск по месту)\n",
        "tools = [TavilySearchResults(max_results=7), weather_tool]"
      ],
      "metadata": {
        "id": "GedOddOdIeUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Используем модель OpenAI для создания агента\n",
        "model = ChatOpenAI(temperature=0.7)  # Температура 0.7 задает умеренную случайность ответов\n",
        "\n",
        "# Создаём агент, который использует инструменты (Tavily для поиска и OpenWeatherMap для погоды)\n",
        "agent = create_openai_functions_agent(model, tools, prompt)\n",
        "\n",
        "# Создаём исполнитель агента\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "PbJmfMn9aMMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример 1: Запрос прогноза погоды\n",
        "print(\"Пример 1: Запрос прогноза погоды\")\n",
        "response = agent_executor.invoke({\"input\": \"будет ли в Москве дождь?\"})\n",
        "print(response)  # Выводим ответ агента на запрос"
      ],
      "metadata": {
        "id": "6M7BE03eBtwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример 2: Запрос о месте через Tavily\n",
        "print(\"\\nПример 2: Запрос о месте для путешествия\")\n",
        "response = agent_executor.invoke({\"input\": \"Какие интересные места для отдыха в Италии?\"})\n",
        "print(response)  # Выводим ответ агента на запрос"
      ],
      "metadata": {
        "id": "kRiai1YTaMBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример 3: Философский вопрос, на который агент отвечает без использования API\n",
        "print(\"\\nПример 3: Философский вопрос\")\n",
        "response = agent_executor.invoke({\"input\": \"Есть ли в жизни счастье?\"})\n",
        "print(response)  # Выводим ответ агента на философский вопрос"
      ],
      "metadata": {
        "id": "g8QgIOSUpSI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReAct Аgent**\n",
        "\n",
        "\n",
        "Action: выбор инструмента\n",
        "\n",
        "Action Input: передача данных в инструмент\n",
        "\n",
        "Observation: наблюдение за ответом\n",
        "\n",
        "Thought: вывод"
      ],
      "metadata": {
        "id": "mG_QpM51byxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReAct agent\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_openai import OpenAI\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/react\")"
      ],
      "metadata": {
        "id": "4DwvXMH2pSFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint as pp\n",
        "a = 'Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'\n",
        "\n",
        "pp(a)"
      ],
      "metadata": {
        "id": "IORKl--Wu2mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_react_agent(model, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "UYi6aeGFpSCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"будет ли в Москве дождь?\"})"
      ],
      "metadata": {
        "id": "2HyA9ViapR47"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}